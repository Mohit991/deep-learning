# -*- coding: utf-8 -*-
"""mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YAbkVSjxkTBt4HEQ7cHpGO_gZjvxl-TY
"""

import tensorflow as tf
mnist = tf.keras.datasets.mnist
(x_train,y_train),(x_test,y_test) = mnist.load_data()
# it contains 28x28 images of hand written numbers from 0-9
# x_train has the training examples that is the input images 
# y_train has the output numbers for those images 0-9


import matplotlib.pyplot as plt
# plt.imshow(x_train[2]) this shows the images in the training set
# print(x_train[0])


# the data is pixel values in gray scale which varies from 0-255, here we need
# to normalize the values, to make them on the same level
x_train = tf.keras.utils.normalize(x_train,axis = 1)
x_test = tf.keras.utils.normalize(x_test,axis = 1)
# plt.imshow(x_train[0])
# print(x_train[0])


# create the model
model = tf.keras.models.Sequential()
# adding the input layer
model.add(tf.keras.layers.Flatten())
# adding hidden layers
# we will choose how many neurons or nodes we want in the hidden layer,
# how many numbers of hidden layers, and activation function for the hidden
# layer
# we will have 2 hidden layers and 128 nodes or neurons in each layer
# we will use ReLU as the activation function for the hidden layers
# this is the most commonly used activation function
# hidden layer 1
model.add(tf.keras.layers.Dense(128,activation = tf.nn.relu))
# hidden layer 2
model.add(tf.keras.layers.Dense(128,activation = tf.nn.relu))
# now we finally add the output layer
# this layer will have nodes equal to the number of classes in the problem
# here we want to identify a number between 0-9 hence
# there are 10 classes so we will have 10 nodes in the output layer
# we will use the softmax function for the output layer for
# probability distribution

model.add(tf.keras.layers.Dense(10,activation = tf.nn.softmax))


# now we enter more details for the model
model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])
# now we are done here
# implementing or training
model.fit(x_train,y_train,epochs = 3)

# lets check the model
# testing
val_loss, val_acc = model.evaluate(x_test,y_test)
print(val_loss,val_acc)

# now we will test it visually
# for that
predictions = model.predict([x_test])
print(predictions)

# predictios is a matrix
print(predictions[0])
# this is a vector for the first image
# this contains 10 values, these values are probability distributions. 
# first value in the vector tells the probability of image being of 1,
# second value tells the probability of it being 2 and so on
# we need to find the maximum of these value 
# which will be the number our model has highest probability of
# for that lets use numpy

import numpy as np
print(np.argmax(predictions[0]))
# it says that it is highest probability that it is a seven
# lets see the image

plt.imshow(x_test[0])
plt.show()
# both mathc
# hence pur neural network is actually working